{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Propensity score matching\n",
    "\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 5 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, preprocessing\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the data in variable and split the data in 2 one for the people that got the training and one for those who do, from now on all the reference to the trained people will be showed by 'NSW' and the untrained will be 'PSID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "data = pd.read_csv(\"lalonde.csv\")\n",
    "#we create a column to have foe each line the name of its group, we crate a new column so that later we can use the \n",
    "#column treat for some calcul.\n",
    "data['group'] = data['treat'].map(lambda x : 'NSW' if (x == 1) else 'PSID') \n",
    "\n",
    "#create the data group of trained people\n",
    "data_NSW = data[data['treat'] == 1]\n",
    "#craete the data group of not trained people\n",
    "data_PSID = data[data['treat'] == 0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse the 2 groups we will begin by calculating their mean and their median to see which group is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the mean of the 2 groups\n",
    "data.groupby('group').mean()['re78']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the median of the 2 groups\n",
    "data.groupby('group').median()['re78']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and median is better for the not trained with its mean being around 10% better and its median being a little les than 20% better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to have a graphical representation of the differences between the trained and untrained. We will use a box graph, but first let us have a little look at the repartition of the 1978 revenue for both groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawRe(all_data, NSW, PSID, on, figure_nb) :\n",
    "    plt.figure(figure_nb)\n",
    "    interval = 1000\n",
    "\n",
    "    # Define bins for sorting the revenues every 1000 $\n",
    "    bins = range(0, int(all_data[on].max()), interval)\n",
    "\n",
    "    # Group both datasets by revenue in the defined bins\n",
    "    groups_N = NSW[on].groupby(np.digitize(NSW[on], bins)).count()\n",
    "    groups_P = PSID[on].groupby(np.digitize(PSID[on], bins)).count()\n",
    "    \n",
    "    # Define an index for the data to display correctly\n",
    "    max_value = max(groups_N.index.values[-1], groups_P.index.values[-1])\n",
    "    index = pd.MultiIndex.from_arrays([range(1, 1 + max_value)], names=[\"test\"])\n",
    "    \n",
    "    # Specify the ticks we want on the graph\n",
    "    ticks = list(map(lambda x : \"\" if x % 5 else str(interval * x), list(range(max_value))))\n",
    "\n",
    "    # Display the sorted values\n",
    "    ax = pd.DataFrame(data={\"Trained\": (groups_N / groups_N.sum()), \\\n",
    "                            \"Untrained\": (groups_P / groups_P.sum())}, index=index \\\n",
    "                            ).plot(kind=\"bar\", figsize=(13, 7))\n",
    "    ax.set_xlabel(on + \" between X and next thousand\")\n",
    "    ax.set_ylabel(\"% of people\")\n",
    "    ax.set_xticklabels(ticks)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawRe(data, data_NSW, data_PSID, 're78', 100).figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do now have a better idea of how the revenue is spread. It seems that the repartition of the revenue is *somewhat* equivalent to a constant factor, but let us use a box graph to get a clear sense of the difference in mean and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a box graph to analyse the 2 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot for figure 101\n",
    "plt.figure(101).clear()\n",
    "\n",
    "#get the earnings of 1978 and sort it\n",
    "data_NSW_val = data_NSW['re78'].sort_values()\n",
    "data_PSID_val = data_PSID['re78'].sort_values()\n",
    "\n",
    "#create a dataframe to put into a graph (once again NSW = trained and PSID = untrained) \n",
    "fuse_Data_re78 = pd.DataFrame(data={'NSW' : data_NSW_val, 'PSID' : data_PSID_val})\n",
    "\n",
    "#plot the graph\n",
    "fuse_Data_re78.plot(kind='box', grid=True).figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph we can see that the PSID is a little better : it has a better 3rd quartile, a better median (we have calculated it before) and if we take out the outliers, it has a best maximum. NSW has a best 1st quartile but the difference is very minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we have seen so far the untrained group is a little better.\n",
    "\n",
    "So a naive \"researcher\" could say that the training does not help or even that it makes things worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete the figure from the cache so that too much figures would not create problem later.\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will study the mean or percentage (depending if a variable is a value or a boolean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the mean of the trained group for the variables\n",
    "data_NSW[['treat', 'age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the mean of the untrained group for the variables\n",
    "data_PSID[['treat', 'age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that treat is only 1 for the trained and only 0 for the untrained (which is normal beause it is its definition), we can see that NSW as a generality is younger, more black, more hispanic, has spend as much time in school, have less chance to have a diploma and have had less earning in 74 and 75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create graph for all the variable to have a better visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph for the distribution of age in the 2 groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot for figure 200\n",
    "plt.figure(200).clear()\n",
    "#plot the graph\n",
    "pd.DataFrame(data={'NSW': data_NSW['age'], 'PSID': data_PSID['age']}).plot(kind='box').figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph for the distribution of year of education in the 2 groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot for figure 201\n",
    "plt.figure(201).clear()\n",
    "#plot the graph\n",
    "pd.DataFrame(data={'NSW': data_NSW['educ'], 'PSID': data_PSID['educ']}).plot(kind='box').figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph for the distribution of earnings in 74 in the 2 groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We plot for figure 202\n",
    "plt.figure(202).clear()\n",
    "#plot the graph\n",
    "pd.DataFrame(data={'NSW': data_NSW['re74'], 'PSID': data_PSID['re74']}).plot(kind='box').figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph for the distribution of earnings in 75 in the 2 groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot for figure 203\n",
    "plt.figure(203).clear()\n",
    "#plot the graph\n",
    "pd.DataFrame(data={'NSW': data_NSW['re75'], 'PSID': data_PSID['re75']}).plot(kind='box').figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph for the percentage of married people in the 2 groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We plot for figure 204\n",
    "plt.figure(204).clear()\n",
    "\n",
    "#create a dataframe of the trained group of the number of people married or not in it\n",
    "married_NSW = data_NSW.married.value_counts().rename({1: \"Married\", 0: \"Unmarried\"})\n",
    "#create a dataframe of the untrained group of the number of people married or not in it\n",
    "married_PSID = data_PSID.married.value_counts().rename({1: \"Married\", 0: \"Unmarried\"})\n",
    "\n",
    "#get the number of married and not married people to a percentage\n",
    "married_NSW *= 100 / married_NSW.sum()\n",
    "married_PSID *= 100 / married_PSID.sum()\n",
    "#plot the graph\n",
    "pd.DataFrame(data={'NSW': married_NSW, 'PSID': married_PSID}).T.plot(kind='barh', stacked=True).set_xlabel(\"%\").figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph for the percentage of people with a diploma in the 2 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot for figure 205\n",
    "plt.figure(205).clear()\n",
    "\n",
    "#create a dataframe of the trained group of the number of people having a diploma or not in it\n",
    "degree_NSW = data_NSW.nodegree.value_counts().rename({1: \"No degree\", 0: \"Degree\"})\n",
    "#create a dataframe of the untrained group of the number of people having a diploma or not in it\n",
    "degree_PSID = data_PSID.nodegree.value_counts().rename({1: \"No degree\", 0: \"Degree\"})\n",
    "#get the number of people having a diploma or not to a percentage\n",
    "degree_NSW *= 100 / degree_NSW.sum()\n",
    "degree_PSID *= 100 / degree_PSID.sum()\n",
    "#plot the graph\n",
    "pd.DataFrame(data={'NSW': degree_NSW, 'PSID': degree_PSID}).T.plot(kind='barh', stacked=True).set_xlabel(\"%\").figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph for the distribution of skin colour between the people in the 2 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot for figure 205\n",
    "plt.figure(205).clear()\n",
    "\n",
    "#get all the column for the races and sum its result getting the number of peope as desribed by the column for the trained group\n",
    "race_NSW = data_NSW[data_NSW.columns[4:6]].sum()\n",
    "#get all the column for the races and sum its result getting the number of peope as desribed by the column for the untrained group\n",
    "race_PSID = data_PSID[data_PSID.columns[4:6]].sum()\n",
    "\n",
    "#append to the 2 groups a final column for the races not talked about in the graph.\n",
    "race_NSW = race_NSW.append(pd.Series(len(data_NSW) - race_NSW.sum(), index=[\"other\"]))\n",
    "race_PSID = race_PSID.append(pd.Series(len(data_PSID) - race_PSID.sum(), index=[\"other\"]))\n",
    "\n",
    "#transform the number of people of each race in a percentage\n",
    "race_NSW *= 100 / race_NSW.sum()\n",
    "race_PSID *= 100 / race_PSID.sum()\n",
    "#plot the graph\n",
    "pd.DataFrame(data={'NSW': race_NSW, 'PSID': race_PSID}).T.plot(kind='barh', stacked=True).set_xlabel(\"%\").figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all those data we can confirm what we have seen with the means that NSW as a generality is younger, more black, more hispanic, has spent as much time in school, have less chance to have a diploma and have had less earning in 74 and 75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our 2 groups are widely different as people and at how much they were winning before.\n",
    "\n",
    "For the analysis of the naive \"researcher\", we can see that the 2 groups being widely different might skew the results we have but worst we see that there is an enormous difference between the earnings before the the training program, so the difference between the 2 groups do not mainly come from the training but from the old earnings, so those data can not be used to test a training program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete the figure from the cache so that too much figures would not create problem later.\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. A propsensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. It's sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explain in the  we use linear_model.LogisticRegression in sklearn to create a logisticRegression model and we give for every person a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the logistic regression model\n",
    "logistic_regressor = linear_model.LogisticRegression()\n",
    "\n",
    "#train the regressor with the data we have\n",
    "#we cast the re78 as int column because the logistic regressor cannot make a regression on continuous values \n",
    "logistic_regressor = logistic_regressor.fit(\n",
    "    data[['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75']].values, \\\n",
    "    data['treat'].values.astype('int'))\n",
    "\n",
    "#create a dataframe to have the propensity score in it\n",
    "propensity_data = data.copy()\n",
    "\n",
    "#add a column re78reg for the propensity score given by the logistic regressor\n",
    "propensity_data['re78reg'] = logistic_regressor.predict_proba(\n",
    "    data[['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75']].values)[:, 1]\n",
    "\n",
    "propensity_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This cell will create the matching between the trained and the untrained dataset. To do that we will use the method max_weight_matching from the library github that you can have more information [here](https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.matching.max_weight_matching.html).\n",
    "\n",
    "This method will take a graph and create a maximum cardinality matching using the weight of the edges. So, to use this, we will make the index of each element of the dataset a node, and we will link every trained index with every untrained index with an edge of weight 1 - absolute difference between the 2 earnings. The reason of this weight is this algorithm os maximizing the weight, and as we want to minimize it we just maximize its inverse (knowing that the idfference between the 2 probability is between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the group of trained people in the dataSet with the propensity\n",
    "propensity_NSW = propensity_data[(propensity_data['treat'] == 1)]\n",
    "#create the group of untrained people in the dataSet with the propensity\n",
    "propensity_PSID = propensity_data[(propensity_data['treat'] == 0)]\n",
    "\n",
    "#the length of the new trained dataset\n",
    "pNSW_length = len(propensity_NSW)\n",
    "#the length of the new untrained dataset\n",
    "pPSID_length = len(propensity_PSID)\n",
    "\n",
    "#create the edges for the graph with weight as discussed in the markdown\n",
    "edges = [(idx, idy, {'weight': 1 - abs(x['re78reg'] - y['re78reg'])}) \\\n",
    "         for (idx, x) in propensity_NSW.iterrows() for (idy, y) in propensity_PSID.iterrows()]\n",
    "\n",
    "#create the graph\n",
    "graph = nx.Graph()\n",
    "#add the edges to the graph\n",
    "graph.add_edges_from(edges)\n",
    "\n",
    "#create the matching in the form of a dictionnary using the method we talked about\n",
    "dico_matching = nx.max_weight_matching(graph, maxcardinality=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dictionnary, we can easily use the fact that the dictionnary has as index all the indexes of the matching to use it to filter the dataset to have only the person of the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to all the individual taht are in the matching\n",
    "matched_data = propensity_data[propensity_data.index.isin(list(dico_matching))]\n",
    "matched_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the matched data\n",
    "matched_NSW = matched_data[matched_data['treat'] == 1]\n",
    "matched_PSID = matched_data[matched_data['treat'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the means of re78\n",
    "matched_data.groupby('group').mean()['re78']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the medians of re78\n",
    "matched_data.groupby('group').median()['re78']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(400).clear()\n",
    "#the repartition of re74 in both groups\n",
    "pd.DataFrame(data={'NSW': matched_NSW['re78'], 'PSID': matched_PSID['re78']}).plot(kind='box', grid=True).figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the means in NSW\n",
    "matched_NSW[['treat', 'age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the means in PSID\n",
    "matched_PSID[['treat', 'age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(401).clear()\n",
    "#the repartition of the age in both groups\n",
    "pd.DataFrame(data={'NSW': matched_NSW['age'], 'PSID': matched_PSID['age']}).plot(kind='box').figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(402).clear()\n",
    "#the repartition of eduction years in both groups\n",
    "pd.DataFrame(data={'NSW': matched_NSW['educ'], 'PSID': matched_PSID['educ']}).plot(kind='box').figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(403).clear()\n",
    "#the repartition of re74 in both groups\n",
    "pd.DataFrame(data={'NSW': matched_NSW['re74'], 'PSID': matched_PSID['re74']}).plot(kind='box').figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(404).clear()\n",
    "#the repartition of re75 in both groups\n",
    "pd.DataFrame(data={'NSW': matched_NSW['re75'], 'PSID': matched_PSID['re75']}).plot(kind='box').figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(405).clear()\n",
    "#we count values in both groups\n",
    "married_NSW = matched_NSW.married.value_counts().rename({1: \"Married\", 0: \"Unmarried\"})\n",
    "married_PSID = matched_PSID.married.value_counts().rename({1: \"Married\", 0: \"Unmarried\"})\n",
    "#we turn them into percents\n",
    "married_NSW *= 100 / married_NSW.sum()\n",
    "married_PSID *= 100 / married_PSID.sum()\n",
    "#we plot the results\n",
    "pd.DataFrame(data={'NSW': married_NSW, 'PSID': married_PSID}).T.plot(kind='barh', stacked=True).set_xlabel(\"%\").figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(406).clear()\n",
    "#we count values in both groups\n",
    "degree_NSW = matched_NSW.nodegree.value_counts().rename({1: \"No degree\", 0: \"Degree\"})\n",
    "degree_PSID = matched_PSID.nodegree.value_counts().rename({1: \"No degree\", 0: \"Degree\"})\n",
    "#we turn them into percents\n",
    "degree_NSW *= 100 / degree_NSW.sum()\n",
    "degree_PSID *= 100 / degree_PSID.sum()\n",
    "#we plot the results\n",
    "pd.DataFrame(data={'NSW': degree_NSW, 'PSID': degree_PSID}).T.plot(kind='barh', stacked=True).set_xlabel(\"%\").figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(407).clear()\n",
    "#we get the racial values\n",
    "race_NSW = matched_NSW[data_NSW.columns[4:6]].sum()\n",
    "race_PSID = matched_PSID[data_PSID.columns[4:6]].sum()\n",
    "#we extract the number of white people\n",
    "race_NSW = race_NSW.append(pd.Series(len(data_NSW) - race_NSW.sum(), index=[\"white\"]))\n",
    "race_PSID = race_PSID.append(pd.Series(len(data_PSID) - race_PSID.sum(), index=[\"white\"]))\n",
    "#we turn them into percents\n",
    "race_NSW *= 100 / race_NSW.sum()\n",
    "race_PSID *= 100 / race_PSID.sum()\n",
    "#we plot the results\n",
    "pd.DataFrame(data={'NSW': race_NSW, 'PSID': race_PSID}).T.plot(kind='barh', stacked=True).set_xlabel(\"%\").figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the repartition of re78 for both groups\n",
    "drawRe(matched_data, matched_NSW, matched_PSID, 're78', 408).figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete the figure from the cache so that too much figures would not create problem later.\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After the matching we have way closer values for the trained and untrained values before the training, the untrained still have a little better earning but as a whole it is as good as equal, so we are satisfied with the matching.\n",
    "\n",
    "This being said the dataset is still not balanced, our main source of concern is that there is a enormous difference in the skin colour between the 2 groups which is a factor that might influence the earning (article on that [here](https://www.theguardian.com/society/2017/sep/26/race-pay-gap-not-just-us-problem-financial-inequality-uk))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data5_list = [matched_data[(matched_data['treat'] == x) & (matched_data['black'] == int(y/2)) & (matched_data['hispan'] == y%2)] \\\n",
    "              for x in range(2) for y in range(3)]\n",
    "groups_5 = ['black PSID', 'hispanic PSID', 'white PSID', 'black NSW', 'hispanic NSW', 'white NSW' ]\n",
    "\n",
    "for_data_group = dict(zip(groups_5, data5_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not use the barplot from there because it would be impossible to read so we use only box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(500).clear()\n",
    "#the repartition of re74 for all three enthnicities in both groups\n",
    "pd.DataFrame(data=dict(zip(groups_5, [x['re74'] for x in data5_list]))).plot(kind='box', figsize=(13, 7)).figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(501).clear()\n",
    "#the repartition of re75 for all three enthnicities in both groups\n",
    "pd.DataFrame(data=dict(zip(groups_5, [x['re75'] for x in data5_list]))).plot(kind='box', figsize=(13, 7)).figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here some big differences between the different group created by example in the earnings of year 74 we can see some extreme difference between the blakc NSW and the white PSID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the means\n",
    "dict(zip(groups_5, [x['re78'].mean() for x in data5_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the medians\n",
    "dict(zip(groups_5, [x['re78'].median() for x in data5_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(600).clear()\n",
    "#the repartition of re78 for all three ethnicities and both groups\n",
    "pd.DataFrame(data=dict(zip(groups_5, [x['re78'] for x in data5_list]))).plot(kind='box', figsize=(13, 7)).figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that in the black and white people group the trained people has better earnings and in the hispanic group the untarined group has better earnings, but kwnowing that there is not a lot of people in the hispanic group (less than  10%) so we can consider that we lack data to have a good reult with this group.\n",
    "\n",
    "So now we can conclude that the training is improving the earning of an individual (especially with black people)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For black people\n",
    "drawRe(matched_data[matched_data['black'] == 1], data5_list[3], data5_list[0], 're78', 601).figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For hispanic people\n",
    "drawRe(matched_data[matched_data['hispan'] == 1], data5_list[4], data5_list[1], 're78', 602).figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For white people\n",
    "drawRe(matched_data[(matched_data['black'] == 0) & (matched_data['hispan'] == 0)], data5_list[5], data5_list[2], 're78', 603).figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete the figure from the cache so that too much figures would not create problem later.\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Applied ML\n",
    "\n",
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from random import randrange\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now download the dataset using the method \"fetch_20newsgroups\" described in the link of the assignement. We will add some degree of randomness for later using the variable random_state of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download the group and save it in a file in our homework\n",
    "newsgroups = fetch_20newsgroups(data_home='20newsgroups', subset='all', random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the TfidfVectorizer described in the assignement, we will now create the features using term frequency–inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the vectorizer to be used to compute the features\n",
    "tfid = TfidfVectorizer()\n",
    "#compute the features of our dataset\n",
    "news_tfid = tfid.fit_transform(newsgroups.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split our newly created TFID dataset in a 80-10-10 split. We do not need to randomize it as it was already randomized when we download it. We will also split the 'target' element of our initial database that represents the category that our model will try to predict. The two split will take the same part of the dataset so that the features match with their category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate the size of one tenth of the original dataset\n",
    "one_tenth = int(len(newsgroups.data) / 10)\n",
    "#transform the dataset into a matrix \n",
    "news_tfid_matrix = news_tfid.A\n",
    "\n",
    "#take the 80% of the mid part of the feature matrix to create the the train dataset of features\n",
    "nt_train = news_tfid_matrix[one_tenth:-one_tenth]\n",
    "#take the 80% of the mid part of the target list to create the the train dataset of categories\n",
    "cat_train = newsgroups['target'][one_tenth:-one_tenth]\n",
    "#take the first one tenth of the feature matrix to create the test dataset of features\n",
    "nt_test = news_tfid_matrix[:one_tenth]\n",
    "#take the first one tenth of the target list to create the test dataset of categories\n",
    "cat_test = newsgroups['target'][:one_tenth]\n",
    "#take the last one tenth of the feature matrix to create the validation dataset of features\n",
    "nt_validation = news_tfid_matrix[-one_tenth:]\n",
    "#take the last one tenth of the target list to create the train validation of categories\n",
    "cat_validation = newsgroups['target'][-one_tenth:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to find the best value for max_depth and n_estimators, because it would take too much time to try to find both at the same time, we will now try to find the best value of max_depth and once it is done, find the best value of n_estimators.\n",
    "\n",
    "Note : with default value, the validation on the test set is 0.64012738853503182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we iterate different value, we use this value to give it to max_depth (with n_estimators being at its default value) to train our classifier and we next see how good it is by comparing the result of our model on the validation set  and the true categorie of the validation set. We have change the value in the range to find the most precise value and let it to the last range check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is not meant to be run as it would take too long to finish, it only show how we find our value.\n",
    "\n",
    "'''\n",
    "for i in range(10, 100, 10):\n",
    "    classifier = RandomForestClassifier(max_depth=i)\n",
    "    classifier.fit(nt_train, cat_train)\n",
    "    print(i, classifier.score(nt_validation, cat_validation))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some different result for a max_depth value : \n",
    "Here are some different result for a max_depth value :\n",
    "10 0.365711252654  \n",
    "20 0.526539278132  \n",
    "30 0.589171974522  \n",
    "40 0.613588110403  \n",
    "10 0.365711252654  \n",
    "20 0.526539278132  \n",
    "30 0.589171974522  \n",
    "40 0.613588110403  \n",
    "50 0.624203821656  \n",
    "60 0.652866242038  \n",
    "70 0.651804670913  \n",
    "80 0.651804670913   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the maximum is attained at max_depth=60 and after that the score goes down and stop changing, so we will keep 60 for the best max_depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we iterate different value, we use this value to give it to n_estimators (with n_estimators being at 120) to train our classifier and we next see how good it is by comparing the result of our model on the validation set and the true categorie of the validation set. We have change the value in the range to find the most precise value and let it to the last range check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is not meant to be run as it would take too long to finish, it only show how we find our value.\n",
    "\n",
    "'''\n",
    "for i in range(10, 100, 10):\n",
    "    classifier = RandomForestClassifier(max_depth=60, n_estimators=i)\n",
    "    classifier.fit(nt_train, cat_train)\n",
    "    print(i, classifier.score(nt_validation, cat_validation))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "10 0.624734607219  \n",
    "20 0.724522292994  \n",
    "30 0.763800424628  \n",
    "40 0.779723991507  \n",
    "50 0.799893842887  \n",
    "60 0.805201698514  \n",
    "70 0.820063694268   \n",
    "80 0.819002123142   \n",
    "90 0.826433121019  \n",
    "100 0.829617834395   \n",
    "150 0.82855626327   \n",
    "200 0.842887473461   \n",
    "250 0.846072186837 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the more estimators there is the more good is our model but the more estimators there is the longer it takes to make the program runs so we will stop at 250 for the value of n_estimators (which takes around 5 hours to finish)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part of the code where we train the true classifier, because it takes too much time, we will save it in a file and not make this part run after it is done.     \n",
    "This code is not meant to be run as it would take too long to finish, it only show how we find our value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#classifier = RandomForestClassifier(max_depth=60, n_estimators=250)\n",
    "#classifier.fit(nt_train, cat_train)\n",
    "\n",
    "#save the classifier in a file so that we can have it again without waiting hours.\n",
    "#joblib.dump(classifier, 'classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the classifier and predict the test dataset.     \n",
    "Warning : you need to dezip the file classifier.pkl.zip for this part to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the classifier from classifier.pkl you need. You need to dezip the file classifier.pkl.zip for this part to work.\n",
    "classifier = joblib.load('classifier.pkl') \n",
    "y_pred = classifier.predict(nt_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how good our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(nt_test, cat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty good score which show how good our parameters  are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(cat_test , y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected after having checked the precision of our model we can see here that for most of the value the class given and the actual class match except for the 1st column wher we can see that a pretty good part of the values mismatch, and the last column where most of the value mismatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the feature importances matrix is too big (around 200 000 elements) we can not just analyse it in its raw form, what we will do is analyse it by orders of magnitude. We will create an array where the 0th position is the numbeer of element with 0 feature importance and the ith element is the number of element with  a feature importance between 10^-i and 10^-(i+1).\n",
    "\n",
    "Note that there is no feature with importance between 0 and 10^-1 so there is no conflict in the counting of the first cell and the minimum feature importance is of order 10^-10 which is why the size of the table is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the array of feature importance\n",
    "feature_importance = classifier.feature_importances_\n",
    "#the array we will use to store the results\n",
    "nb_element_by_order = [0] * 10\n",
    "\n",
    "#for all element of the feature importance array if it is 0 we put +1 at place 0 \n",
    "#and else we put +1 at its order of magnitude\n",
    "for x in feature_importance :\n",
    "    if x == 0 :\n",
    "        nb_element_by_order[0] += 1\n",
    "    else :\n",
    "        nb_element_by_order[int(-log10(x))] += 1\n",
    "\n",
    "print(nb_element_by_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are know going to look at how important are the best 0.1% (around 174) to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_feature_importance = sorted(feature_importance)\n",
    "sum(sort_feature_importance[-174:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that most of the element (around 2/3) has no importance and a lot have very few importance on the other hand we can see that the best 0.1% features make 25% of the importance of the model.\n",
    "\n",
    "This means that for the classifier, most of the element are of no importance and that only a litle number of case allow him to build the tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
