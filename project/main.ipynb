{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work done till this second milestone was mainly to get our data and clean them. As we do not use a provided dataset, collecting and cleaning data take a tremendeous amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import json\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The votations have been scraped from the BFS website. See the notbook 'bfs_scrap' for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read votation data\n",
    "df = pd.read_pickle(\"data/votations.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that capitalizes the first letter in a given string\n",
    "def cap_first(s):\n",
    "    return s[0].capitalize() + s[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the indices of the votation data\n",
    "data = pd.DataFrame([x for x, _ in df.index.values]).drop_duplicates()\n",
    "\n",
    "# Rename the community column\n",
    "data.columns = [\"Commune\"]\n",
    "\n",
    "# Create columns for districts/cantons/countries\n",
    "data[\"District\"] = np.nan\n",
    "data[\"Canton\"] = np.nan\n",
    "data[\"Pays\"] = np.nan\n",
    "\n",
    "# Extract names of districts/cantons/countries\n",
    "data[\"Pays\"] = data[\"Commune\"].map(lambda x : x if x[0] != \">\" and x[0] != \"-\" and x[0] != \".\" else np.nan)\n",
    "data[\"Canton\"] = data[\"Commune\"].map(lambda x : x[2:] if x[0] == \"-\" else np.nan)\n",
    "data[\"District\"] = data[\"Commune\"].map(lambda x : x[3:] if x[0] == \">\" else np.nan)\n",
    "\n",
    "# Propagate names of districts/cantons/countries downwards\n",
    "data = data.fillna(method='ffill')\n",
    "\n",
    "# Remove lines that do not describe a community\n",
    "data = data[data[\"Commune\"].map(lambda x : x[0] == \".\")]\n",
    "\n",
    "# Clean canton and district names\n",
    "data[\"Canton\"] = data[\"Canton\"].map(lambda x : x if x is np.nan else x.split(\" /\")[0])\n",
    "data[\"District\"] = data[\"District\"].map(lambda x :\n",
    "                                        x if \\\n",
    "                                            \"Bezirk See\" in x else \\\n",
    "                                        \"\".join(x.split(\"'\")[1:]).strip() if \\\n",
    "                                            \"District d'\" in x or \\\n",
    "                                            \"District de l'\" in x else\n",
    "                                        cap_first(\" \".join(x.split(\" \")[2:])).strip() if \\\n",
    "                                            \"Arrondissement administratif\" in x or \\\n",
    "                                            \"District\" in x or \\\n",
    "                                            \"Canton\" in x or \\\n",
    "                                            \"Distretto di\" in x else \\\n",
    "                                        \" \".join(x.split(\" \")[1:]).strip() if \\\n",
    "                                            \"Verwaltungskreis\" in x or \\\n",
    "                                            \"Wahlkreis\" in x or \\\n",
    "                                            \"Kanton\" in x or \\\n",
    "                                            \"Bezirk\" in x or \\\n",
    "                                            \"Region\" in x \\\n",
    "                                        else x)\n",
    "\n",
    "data[\"District\"] = data[\"District\"].map(lambda x : \\\n",
    "                                        \"Obwald\" if x == \"Obwalden\" else \\\n",
    "                                        \"Nidwald\" if x == \"Nidwalden\" else \\\n",
    "                                        x)\n",
    "\n",
    "# Write correct district/canton/country data for foreign votes\n",
    "data[[\"District\", \"Canton\", \"Pays\"]] = data.apply(lambda x : pd.Series([\"-\", \"-\", \"Etranger\"]) if \\\n",
    "                                        \"-Ausland-\" in x[\"District\"] or \\\n",
    "                                        \" de l'étranger\" in x[\"District\"] or \\\n",
    "                                        \"-Korrespondenzweg\" in x[\"District\"] or \\\n",
    "                                        \"-autres\" in x[\"District\"] or \\\n",
    "                                        \"-voto per corrispondenza\" in x[\"District\"] \\\n",
    "                                      else pd.Series([x[\"District\"], x[\"Canton\"], x[\"Pays\"]]), axis=1)\n",
    "\n",
    "#data.set_index('Commune', inplace=True)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "#NAMES NEED TO BE CLEANED AFTER MODIFYING THE PROPER DATA, OTHERWISE COMMUNITY NAMES WILL NOT MATCH\n",
    "#NO DISTRICTS FOR GENEVA, SCHAFFHAUSEN, APPENZELL INNERRHODEN, OBWALD AND NIDWALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for districts/cantons/countries\n",
    "\n",
    "df = pd.read_pickle(\"data/votations.pkl\")\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df = df.merge(data, on=\"Commune\")\n",
    "df[\"Commune\"] = df[\"Commune\"].map(lambda x : x[7:] if x[0] == \".\" else x)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace ' (Urne commune)' in the names of towns\n",
    "\n",
    "def remove_urne_commune(x):\n",
    "    if ' (Urne commune)' in x:\n",
    "        return x[:-len(' (Urne commune)')]\n",
    "    else: return x\n",
    "    \n",
    "df['Commune'] = df['Commune'].apply(lambda x : remove_urne_commune(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'Etranger' and save to pickle\n",
    "df = df[df['Pays'] == 'Suisse']\n",
    "df.drop(['Pays'], axis=1, inplace=True)\n",
    "df.to_pickle(\"data/data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THEMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = pd.read_csv(\"data/px-x-1703010000_103.csv\", sep=\";\", encoding=\"cp1254\", skiprows=2)[:-1]\n",
    "themes = themes[~themes['Période'].str.contains(\"bis\")]\n",
    "themes[\"Période\"] = themes[\"Période\"].apply(lambda x : x.split(\" \")[1])\n",
    "themes = themes.set_index(\"Période\")\n",
    "themes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We extract the subjects of votations from the index of the data and make a dataframe we the subjects and the year\n",
    "df = pd.read_pickle(\"data/votations.pkl\")\n",
    "votations = pd.DataFrame(df.index.levels[1])\n",
    "votations[\"Année\"] = votations[\"Votation\"].apply(lambda s : s[6:10])\n",
    "votations = votations.sort_values(\"Année\")\n",
    "votations = votations.reset_index(drop=True)\n",
    "\n",
    "header = list(themes.columns)\n",
    "values = []\n",
    "\n",
    "#Now let's use our indexation of themes to append a theme to each votation. We use a handmade indexation of the\n",
    "#themes since no mapping between subjects and themes exist online. We used the available listing of voted themes\n",
    "#for each year and manually reattributed the themes to the subjects.\n",
    "with open(\"data/theme_indices.txt\", \"r\") as file:\n",
    "    for year in votations[\"Année\"].unique():\n",
    "        indices = file.readline().replace(\" \", \"\").split(\",\")\n",
    "        temp = []\n",
    "        \n",
    "        #We get the themes for the current year\n",
    "        for i in range(len(header)):\n",
    "            for j in range(int(themes.loc[year, header[i]])):\n",
    "                temp.append(header[i])\n",
    "\n",
    "        #We reorder them using our indexation and add them to the list\n",
    "        values += list(map(lambda x : temp[int(x) - 1], indices))\n",
    "        \n",
    "#We add the list to the dataframe as a column\n",
    "votations[\"Thématique\"] = values\n",
    "votations = votations.drop(\"Année\", axis=1)\n",
    "votations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votations.to_pickle(\"data/Thématique.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominating language in towns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to prove that a Röstigraben exists, we need the language spoken in each town together with the votation results. We find such data [here](https://www.atlas.bfs.admin.ch/maps/13/fr/12401_229_228_227/20443.html). The raw values downloaded from there can be found in **data/3007\\_Langues\\_nationales\\_dominantes\\_dans\\_les\\_communes\\_en\\_2000\\_(fr)**. It gives the dominating language for each town in 2000. The value each town gets is [Language]: [(medium)/(strong)] or \"No domination\" (note that these values are in French and language can be either one of the fourth national language, i.e, 'French', 'German', 'Italian', or 'Romansh').\n",
    "\n",
    "We had a lot of mismatches between the town names in the votations dataframe and the file above. The reason is that mergers between multiple town have been encouraged from the beginning of this century. While the votation data have been nicely adapted susequently by the BFS, it is obviously not the case for the last language survey performed in 2000. We manually added the merge in the file, resulting in **data/'data/languages_2000.xlsx'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = pd.read_excel('data/languages_2000.xlsx', skiprows=1, skip_footer=11)\n",
    "languages.drop(['Regions-ID'], axis=1, inplace=True)\n",
    "languages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading votation with only town as index and extracting list of towns\n",
    "df_votation = pd.read_pickle('data/temp_data.pkl')\n",
    "towns_votations = set(df_votation[df_votation['Pays'] == 'Suisse']['Commune'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all unique town in the language dataset  \n",
    "towns_languages = set(languages['Regionsname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the difference between the two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1 = towns_votations - towns_languages\n",
    "len(diff1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --> 100% match between towns in votations and towns in languages after a tough job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to draw a map of the languages with folium but without success. However, the code we tried to run can be seen in the notebook 'parse_languages'. You can also find in that notebook some helpers to perform the manual matching of the towns name in the language file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map by theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part creates maps for each that show the percentage of agreement for each thematique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data for the % by commune and for the the theme of each votation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"data/data.pkl\")\n",
    "thematique = pd.read_pickle(\"data/Thématique.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the json to create the commune on the map, and create the lsit of the commune in the json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switzerland_coord = [46.765213, 8.252444]\n",
    "town_geo_path = r'data/switzerland_borders/admin_level_8.geojson'\n",
    "geo_json_data = json.load(open(town_geo_path, encoding=\"utf8\"))\n",
    "commune = [x['name']  for x in geo_json_data['features']]\n",
    "\n",
    "thematique.to_pickle('data/data_theme.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the 2 dataframes so that we have for each votation and each commune the theme and the percentage of yes. We only takes 'Thématique','Commune' and 'Oui en %' because it will be the only usefull information for later (Votation is no longer use full once we managed to merge)\n",
    "\n",
    "We also make sure thatthere is in the dtaframes only commune that are in the json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = data.merge(thematique , on = 'Votation')[['Thématique','Commune','Oui en %']]\n",
    "data_t = data_t[data_t['Commune'].isin(commune)]\n",
    "\n",
    "\n",
    "data_t.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the data by theme, and for each one we create a map showing how much people voted yes. We then save it into an html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for theme, data_theme in data_t.groupby('Thématique') :\n",
    "    data_theme = data_theme.groupby('Commune', as_index=False).mean()\n",
    "    map1 = folium.Map(location=switzerland_coord, zoom_start=8)\n",
    "    map1.choropleth(geo_data = geo_json_data, \\\n",
    "                    data = data_theme, \\\n",
    "                    columns = ['Commune', 'Oui en %'], \\\n",
    "                    key_on = 'feature.name', \\\n",
    "                    fill_color = 'RdYlGn', \\\n",
    "                    fill_opacity = 0.7, \\\n",
    "                    line_opacity = 0.2, \\\n",
    "                    legend_name = 'yes in % given to the theme ' + theme)\n",
    "    \n",
    "    map1.save('data/map_theme/map_' + theme + '.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we will randomly take 3 theme to make an alysis (we took only 3 but the anylisis can be the same on the other maps.\n",
    "\n",
    "So we will look at the ['Economic'](data/map_theme/map_Economie.html) theme, ['Politique de securite'](data/map_theme/map_Politique de sécurité.html) theme and the ['Régime politique'](data/map_theme/map_Régime politique.html) :    \n",
    "In each one of them we can clearly see a large strip beggining in the valais and ending at Vaduz. We can see in this the Röstigraben, but at the same time the north part and east part of the deutsch part of Switzerland are not as different as the strip, so even if we can put forward the split between the freanch part and deutsch part, it is possible that the Röstigraben is not the only explanationfor those differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map by recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for using the proposition of vote of each poilitical party to create a visual representation of how much each party is listened and try to see which region vote more for each party."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part load the dataframe with the % of yes and clean the dataframe so that we have only the date in one column and another to have only the subject of the votation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataframe with the votation data\n",
    "df = pd.read_pickle(\"data/data.pkl\")\n",
    "df['Date'] = df['Votation'].map(lambda x : x.split(' ')[0])\n",
    "df['Votation'] = df['Votation'].map(lambda x : ' '.join(x.split(' ')[1:]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download all the page \n",
    "xls = pd.ExcelFile('data/Recommandations des partis.xls')\n",
    "#the dataframe to fill with the info of the recommandation\n",
    "recommend = pd.DataFrame()\n",
    "\n",
    "#for every page with information from 2017 to 1981  :\n",
    "for i in range ((2017 - 1981 + 1)) :\n",
    "    #get the page i\n",
    "    x = xls.parse(i)\n",
    "    #change the name of the columns so that they will be easier to use \n",
    "    x.columns = range(len(x.columns))\n",
    "    \n",
    "    #We will need the parties and the numero of the votation, for that we use the fact that the line that have the date \n",
    "    #always begin by 'Parti 1)' and that the no of votation is 2 lines later\n",
    "    base_nb = x[x.iloc[:, 0] == 'Parti 1)'].index[0]\n",
    "    \n",
    "    #We drop all the columns with only Nan\n",
    "    x = x.dropna(axis=1, how='all')\n",
    "\n",
    "    #concat the date, the No of votation and the conseil of vote using the fact that they are the only lines with Nan\n",
    "    recommend_inter = pd.concat([x.iloc[base_nb:base_nb + 1], x.iloc[base_nb + 2:base_nb + 3], x.dropna()]).transpose()\n",
    "    \n",
    "    #this part change the name of the party so that even if there is little change in the name  (like had a 3) to the name)\n",
    "    #we still have consistent name\n",
    "    recommend_inter.iloc[0, 2:] = recommend_inter.iloc[0, 2:].map(lambda x : x.split(' ')[0])    \n",
    "    \n",
    "    #change the name of the columns so that they will be easier to use\n",
    "    recommend_inter.columns = range(len(recommend_inter.columns))\n",
    "    \n",
    "    recommend_inter.iloc[:, 0] = recommend_inter.iloc[:, 0] + str(2017 - i)\n",
    "    # make sure that 'no ###' are  made into 'No ###'\n",
    "    recommend_inter.iloc[:, 1] = recommend_inter.iloc[:, 1].map(lambda x : x[0].upper() + x[1:], na_action='ignore')\n",
    "    \n",
    "    #get the lines that give the name of a votation knowing its No\n",
    "    propositions = x[x.iloc[:, 0].str.contains('No ').fillna(False)]\n",
    "    #change the name of the columns so that they will be easier to use\n",
    "    propositions.columns = range(len(propositions.columns))\n",
    "    \n",
    "    #The next part is transforming the No of votation into name of votation\n",
    "    \n",
    "    #in some sheet this information is on only 1 cell so we split the information in 2 clls like the rest of the sheets \n",
    "    if (propositions.iloc[:, 1].isnull()).all():\n",
    "        if (2017 - i == 1997):\n",
    "            propositions.iloc[:, 1] = propositions[0].map(lambda x : x.split(' ')[1])\n",
    "            propositions.iloc[:, 0] = propositions[0].map(lambda x : x.split(' ')[0].rstrip())\n",
    "        else:\n",
    "            propositions.iloc[:, 1] = propositions[0].map(lambda x : x.split(':')[1][1:])\n",
    "            propositions.iloc[:, 0] = propositions[0].map(lambda x : x.split(':')[0].rstrip())\n",
    "    \n",
    "    \n",
    "    dico_no_propos = propositions.dropna(1).set_index(0).to_dict()\n",
    "    \n",
    "    #some dico are inside a dictionarry {1:{true_dictionarry}}\n",
    "    if (dico_no_propos.get(1) == None) :\n",
    "        recommend_inter.iloc[:, 1] = recommend_inter.iloc[:, 1].map(dico_no_propos)\n",
    "    else :\n",
    "        recommend_inter.iloc[:, 1] = recommend_inter.iloc[:, 1].map(dico_no_propos[1])\n",
    "        \n",
    "    #rename column meaningfully\n",
    "    recommend_columns = ['Date', 'Votation'] + list(recommend_inter.iloc[0][2:])\n",
    "    recommend_inter.columns = recommend_columns\n",
    "    \n",
    "    #fill the date for the line that does not have it\n",
    "    recommend_inter['Date'] = recommend_inter['Date'].fillna(method='ffill')\n",
    "    #transpose so that we can join\n",
    "    recommend_inter = recommend_inter.transpose()\n",
    "    \n",
    "    #delete duplicate (in some sheet there is twice PST )\n",
    "    recommend_inter = recommend_inter.groupby(recommend_inter.index).first()\n",
    "    #join the 2 dataframe, the name of the column are not important\n",
    "    recommend = recommend.join(recommend_inter, how='outer', lsuffix='l', rsuffix='r')\n",
    "\n",
    "recommend = recommend.transpose()  \n",
    "\n",
    "#clean the dataframe \n",
    "recommend = recommend[~recommend.loc[:, 'Date'].str.contains('Parti 1').fillna(False)]\n",
    "recommend = recommend[~recommend['Votation'].isnull()]\n",
    "#create meaningfull index\n",
    "recommend.index = range(len(recommend.index))\n",
    "\n",
    "#transform the date into the same format than the main dataframe\n",
    "month_to_int = { \\\n",
    "    'janvier' : '01', \\\n",
    "    'février' : '02', \\\n",
    "    'févirer' : '02', \\\n",
    "    'mars' : '03', \\\n",
    "    'avril' : '04', \\\n",
    "    'mai' : '05', \\\n",
    "    'juin' : '06', \\\n",
    "    'juillet' : '07', \\\n",
    "    'aout' : '08', \\\n",
    "    'septembre' : '09', \\\n",
    "    'octobre' : '10', \\\n",
    "    'novembre' : '11', \\\n",
    "    'décembre' : '12' \\\n",
    "}\n",
    "\n",
    "def good_format_month (x) :\n",
    "    if '.' in x :\n",
    "        split_x = x.split('.')\n",
    "        return split_x[0].zfill(2) + '.' + split_x[1].zfill(2) + '.' + split_x[2][:4]\n",
    "    else :\n",
    "        return str(x.split(' ')[0]).zfill(2) + '.' +  month_to_int[str(x.split(' ')[1]) ] + '.' + x[-4:] \n",
    "\n",
    "recommend.loc[:, 'Date'] = recommend.loc[:, 'Date'].map(lambda x : good_format_month(x))\n",
    "\n",
    "recommend = recommend.loc[:, recommend.columns.drop_duplicates() ]\n",
    "#parties is the list of all the parties\n",
    "parties = list(recommend.columns.drop_duplicates())\n",
    "parties.remove('Date')\n",
    "parties.remove('Votation')\n",
    "parties.remove('Parti')\n",
    "\n",
    "recommend = recommend.loc[:, ['Date', 'Votation'] + parties]\n",
    "recommend.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value so that they are all uniform       \n",
    "a yes recommandation becomes 1     \n",
    "a no recommandation becomes -1   \n",
    "No data or no recommendation is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_choice(x) :\n",
    "    if (x == 1 or str(x) == 'oui') :\n",
    "        return 1\n",
    "    elif (x == 2 or str(x) == 'non'):\n",
    "        return -1\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "good_recommend = recommend.copy() \n",
    "good_recommend.loc[:, parties] = good_recommend.loc[:, parties].applymap(lambda x : translate_choice(x))\n",
    "good_recommend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a dictionarry to link the name of the votation in the recomendation dataframe and the vote dataframe.   \n",
    "The problem is that the name are very different, so to link one to another we need to group the votation of the 2 by date and for each date we compare all pair using SequenceMatch, we find the best match, we decide to link this pair together, we take out the 2 element from their respective list and we research a new maximum until there is no elemnt in one of the list.\n",
    "\n",
    "This method might create some imprecision but as a whole it is solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_recom_vote = {}\n",
    "\n",
    "for date1, group1 in df[df['Commune'] == 'Aeugst am Albis'].groupby('Date'):\n",
    "    \n",
    "    for date2, group2 in good_recommend.groupby('Date'):\n",
    "        \n",
    "        if (date1 == date2) :\n",
    "            vot1 = list(group1['Votation']) \n",
    "            vot2 = list(group2['Votation'])\n",
    "            \n",
    "            while (len(vot1) > 0 and len(vot2) > 0):\n",
    "                max_v = -1\n",
    "                max_match = [0, 0]\n",
    "                \n",
    "                for elem1 in vot1:\n",
    "                    for elem2 in vot2:\n",
    "                        current_v = SequenceMatcher(None, elem1, elem2).ratio()\n",
    "                        \n",
    "                        if (current_v > max_v) :\n",
    "                            max_v = current_v\n",
    "                            max_match = [elem1, elem2]\n",
    "                            \n",
    "                dico_recom_vote[max_match[0]] = max_match[1]\n",
    "                dico_recom_vote[max_match[1]] = max_match[0]\n",
    "                vot1.remove(max_match[0])\n",
    "                vot2.remove(max_match[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a recommandation dataframe with names corresponding to the ones in the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_to_join = good_recommend.copy()\n",
    "recommend_to_join['Votation'] = recommend_to_join['Votation'].map(dico_recom_vote)\n",
    "recommend_to_join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the map :    \n",
    "get the json to do the border   \n",
    "get all the commune name     \n",
    "only keep the value that ar ein the json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switzerland_coord = [46.765213, 8.252444]\n",
    "town_geo_path = r'data/switzerland_borders/admin_level_8.geojson'\n",
    "geo_json_data = json.load(open(town_geo_path, encoding=\"utf8\"))\n",
    "commune = [x['name'] for x in geo_json_data['features']]\n",
    "\n",
    "to_map = df.merge(recommend_to_join.loc[:, ['Votation'] + parties], on='Votation')\n",
    "to_map = to_map[to_map['Commune'].isin(commune)]\n",
    "to_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a map of % of people agreeing in a party for each party.\n",
    "\n",
    "People agreeing are considered to people that vote the same as the party if it votes yes or no. We do not take into consideration other proposition of the party (like abstentation) or when we do not have information about the recommandation of a party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parti in parties :\n",
    "    current_to_map = to_map.loc[:, ['Commune', 'Oui en %', parti]]\n",
    "    current_to_map = current_to_map[current_to_map[parti] != 0]\n",
    "    current_to_map['Agreement'] = current_to_map[['Oui en %', parti]] \\\n",
    "        .apply(lambda x : x['Oui en %'] if x[parti] == 1 else 100 - x['Oui en %'], axis=1)\n",
    "    \n",
    "    current_to_map = current_to_map.groupby('Commune', as_index=False).mean()\n",
    "    \n",
    "    map1 = folium.Map(location=switzerland_coord, zoom_start=8)\n",
    "    map1.choropleth(geo_data = geo_json_data, \\\n",
    "                    data = current_to_map, \\\n",
    "                    columns = ['Commune', 'Agreement'], \\\n",
    "                    key_on = 'feature.name', \\\n",
    "                    fill_color = 'RdYlGn', \\\n",
    "                    fill_opacity = 0.7, \\\n",
    "                    line_opacity = 0.2, \\\n",
    "                    legend_name = 'Agreement in % with ' + parti)\n",
    "    \n",
    "    map1.save('data/maps_partis/map_' + parti + '.html')\n",
    "    print(map1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take 4 parties to make an alysis, the 2 biggest (UDC,PS) , on with a medium level of importance (PB) and a parti with very few seat at the parlement (PST) (once again we took only 4 but the anylisis can be the same on the other maps) .\n",
    "\n",
    "So we will look at the ['UDC'](data//maps_partis/map_UDC.html), ['PS](data/maps_partis/map_PS.html) , ['PB](data/maps_partis/map_PBD.html) and the ['PST'](data/maps_partis/map_PST.html) :    \n",
    "This time the maps shows a lot of difference : the PS and PST shows very clear distinction between the french part and the deutsch part, very fitting of the Röstigraben, in the UDC and PBC on the other hand the difference are less important where in the UDC the difference overall between most of the state is not that big and in the PBC map the Röstigraben is near impossible to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In the end we have seen obvious differences between the french part and the deutsch part, but those differences seems to vary, not apply every where, and even in those differences we can see that they are far from being uniform.     \n",
    "As we stand it is diffcult to approve or deny the existence of the Röstigraben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
