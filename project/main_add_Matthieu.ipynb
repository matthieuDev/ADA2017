{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main notebook, it is used to create the map and any data we need for the website.\n",
    "\n",
    "We will not explain in this notebook the result obtained or the reason why we want them, those will be discuss in the data story.\n",
    "Please do not try to run this notebook in a unchronological order otherwise it might not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import folium\n",
    "\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "#This 2 library are used for clustering the data\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "#This library is used to reduce a vector in such way as to retain as much data as possible\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read votation data\n",
    "data = pd.read_pickle(\"data/votations.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that capitalizes the first letter in a given string\n",
    "def cap_first(s):\n",
    "    return s[0].capitalize() + s[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the indices of the votation data\n",
    "data = pd.DataFrame([x for x, _ in data.index.values]).drop_duplicates()\n",
    "\n",
    "# Rename the community column\n",
    "data.columns = [\"Commune\"]\n",
    "\n",
    "# Create columns for districts/cantons/countries\n",
    "data[\"District\"] = np.nan\n",
    "data[\"Canton\"] = np.nan\n",
    "data[\"Pays\"] = np.nan\n",
    "\n",
    "# Extract names of districts/cantons/countries\n",
    "data[\"Pays\"] = data[\"Commune\"].map(lambda x : x if x[0] != \">\" and x[0] != \"-\" and x[0] != \".\" else np.nan)\n",
    "data[\"Canton\"] = data[\"Commune\"].map(lambda x : x[2:] if x[0] == \"-\" else np.nan)\n",
    "data[\"District\"] = data[\"Commune\"].map(lambda x : x[3:] if x[0] == \">\" else np.nan)\n",
    "\n",
    "# Propagate names of districts/cantons/countries downwards\n",
    "data = data.fillna(method='ffill')\n",
    "\n",
    "# Remove lines that do not describe a community\n",
    "data = data[data[\"Commune\"].map(lambda x : x[0] == \".\")]\n",
    "\n",
    "# Clean canton and district names\n",
    "data[\"Canton\"] = data[\"Canton\"].map(lambda x : x if x is np.nan else x.split(\" /\")[0])\n",
    "data[\"District\"] = data[\"District\"].map(lambda x :\n",
    "                                        x if \\\n",
    "                                            \"Bezirk See\" in x else \\\n",
    "                                        \"\".join(x.split(\"'\")[1:]).strip() if \\\n",
    "                                            \"District d'\" in x or \\\n",
    "                                            \"District de l'\" in x else\n",
    "                                        cap_first(\" \".join(x.split(\" \")[2:])).strip() if \\\n",
    "                                            \"Arrondissement administratif\" in x or \\\n",
    "                                            \"District\" in x or \\\n",
    "                                            \"Canton\" in x or \\\n",
    "                                            \"Distretto di\" in x else \\\n",
    "                                        \" \".join(x.split(\" \")[1:]).strip() if \\\n",
    "                                            \"Verwaltungskreis\" in x or \\\n",
    "                                            \"Wahlkreis\" in x or \\\n",
    "                                            \"Kanton\" in x or \\\n",
    "                                            \"Bezirk\" in x or \\\n",
    "                                            \"Region\" in x \\\n",
    "                                        else x)\n",
    "\n",
    "data[\"District\"] = data[\"District\"].map(lambda x : \\\n",
    "                                        \"Obwald\" if x == \"Obwalden\" else \\\n",
    "                                        \"Nidwald\" if x == \"Nidwalden\" else \\\n",
    "                                        x)\n",
    "\n",
    "# Write correct district/canton/country data for foreign votes\n",
    "data[[\"District\", \"Canton\", \"Pays\"]] = data.apply(lambda x : pd.Series([\"-\", \"-\", \"Etranger\"]) if \\\n",
    "                                        \"-Ausland-\" in x[\"District\"] or \\\n",
    "                                        \" de l'étranger\" in x[\"District\"] or \\\n",
    "                                        \"-Korrespondenzweg\" in x[\"District\"] or \\\n",
    "                                        \"-autres\" in x[\"District\"] or \\\n",
    "                                        \"-voto per corrispondenza\" in x[\"District\"] \\\n",
    "                                      else pd.Series([x[\"District\"], x[\"Canton\"], x[\"Pays\"]]), axis=1)\n",
    "\n",
    "#data.set_index('Commune', inplace=True)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "#NAMES NEED TO BE CLEANED AFTER MODIFYING THE PROPER DATA, OTHERWISE COMMUNITY NAMES WILL NOT MATCH\n",
    "#NO DISTRICTS FOR GENEVA, SCHAFFHAUSEN, APPENZELL INNERRHODEN, OBWALD AND NIDWALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for districts/cantons/countries\n",
    "\n",
    "df = pd.read_pickle(\"data/votations.pkl\")\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "data = df.merge(data, on=\"Commune\")\n",
    "data[\"Commune\"] = data[\"Commune\"].map(lambda x : x[7:] if x[0] == \".\" else x)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace ' (Urne commune)' in the names of towns\n",
    "\n",
    "def remove_urne_commune(x):\n",
    "    if ' (Urne commune)' in x:\n",
    "        return x[:-len(' (Urne commune)')]\n",
    "    else: return x\n",
    "    \n",
    "data['Commune'] = data['Commune'].apply(lambda x : remove_urne_commune(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Commune'] != 'Meienried']\n",
    "data = data[data['Commune'] != 'Hellsau']\n",
    "data = data[data['Commune'] != 'Rüti bei Lyssach']\n",
    "data = data[data['Commune'] != 'Deisswil bei Münchenbuchsee']\n",
    "data = data[data['Commune'] != 'Clavaleyres']\n",
    "data = data[data['Commune'] != 'Jaberg']\n",
    "data = data[data['Commune'] != 'Niedermuhlern']\n",
    "\n",
    "data = data.replace(to_replace='Wald (BE)', value='Wald (BE)-Niedermuhlern')\n",
    "data = data.replace(to_replace='Kirchdorf (BE)', value='Kirchdorf (BE)-Jaberg')\n",
    "data = data.replace(to_replace='Münchenwiler', value='Münchenwiler-Clavaleyres')\n",
    "data = data.replace(to_replace='Wiggiswil', value='Wiggiswil-Deisswil bei Münchenbuchsee')\n",
    "data = data.replace(to_replace='Mötschwil', value='Mötschwil-Rüti bei Lyssach')\n",
    "data = data.replace(to_replace='Höchstetten', value='Höchstetten-Hellsau')\n",
    "data = data.replace(to_replace='Büren an der Aare', value='Büren an der Aare-Meienried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'Etranger' and save to pickle\n",
    "data = data[data['Pays'] == 'Suisse']\n",
    "data.drop(['Pays'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is loading all the data we need to draw the map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coordinate for folium of the center of Switzerland\n",
    "switzerland_coord = [46.765213, 8.252444]\n",
    "\n",
    "#path to a geojson with in it all the borders between cantons and the swiss border.\n",
    "town_geo_path = r'data/switzerland_borders/municipalities_no_urnes.geojson'\n",
    "#content of the geojson \n",
    "geo_json_data = json.load(open(town_geo_path, encoding=\"utf8\"))\n",
    "#list of the name of all the commune into the geojson\n",
    "commune = [x['name']  for x in geo_json_data['features']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is creating a matrix that represent the dataframe data with a line representing a commune and a column a votation.\n",
    "This will be used when we will need to clustre our data.\n",
    "It is possible to pass from a commune to its line on the matrix and the other way around by using respectively commune_dict (which is a dictionary commune to index) and commue_list. It is the same for each vote with votation_dict and votation_list.\n",
    "\n",
    "If we are missing some data for the matrix we put this vote of a canton to 50% because we cannot let a empty case for the clustering and this way the votation will be counting as not really having an opinion about the vote. (This case will happen 1306 on 696696 so this will not skew the resuts too much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commune_list = list(set(data['Commune'].values))\n",
    "commune_dict = { val : idx for idx , val in enumerate(commune_list)   }\n",
    "\n",
    "votation_list = list(set(data['Votation'].values))\n",
    "votation_dict = { val : idx for idx , val in enumerate(votation_list)   }\n",
    "\n",
    "# we create an array of the good size and for each line we get the the result of the votation in the good position,\n",
    "#using the dictionaries to find the good indexes.\n",
    "X = np.ones((len(commune_list) , len(votation_list) ) , dtype=float)\n",
    "for x in data [['Commune','Votation','Oui en %']].fillna(50).values :\n",
    "    X [commune_dict[x[0]]][votation_dict[x[1]]] = x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is a color list that will be used to colour the different group in the maps where the distinction we want to do are about a few discrete values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [ '#9ecae1' , '#2171b5' , '#a1d99b' , '#238b45' , '#fdae6b' , '#6a51a3' , '#bd0026' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw map languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map is about creating a map which show what language is talked in this commune and at which intensity, and it will show the most voted and less voted proposition by language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads the language spoken by commune, for each commune we gives the information of the language spoken (french, german, italian or romansh) and at which intensity it is spoken (big or medium) or if no language is a majority. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = pd.read_excel('data/languages_2000.xlsx', skiprows=1, skip_footer=11)\n",
    "languages.drop(['Regions-ID'], axis=1, inplace=True)\n",
    "languages.columns = ['Commune' , 'Language']\n",
    "languages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give for each line of data the language spoken inside its commune. \n",
    "data_lang = data.merge ( languages , on = 'Commune')\n",
    "data_lang.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a dictionnary that associate a language to a color on the map.\n",
    "color_language = {\n",
    "    'Allemand: forte':'red',\n",
    "    'Allemand: moyenne':'lightcoral',\n",
    "    'Français: forte':'blue',\n",
    "     'Français: moyenne':'lightskyblue',\n",
    "    'Italien: forte':'limegreen',\n",
    "     'Italien: moyenne':'darkseagreen',\n",
    "    'Romanche: forte':'yellow',\n",
    "    'Romanche: moyenne':'khaki',\n",
    "    'Pas de dominance nette': 'grey'   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is drawing the map of the language in Switzerland\n",
    "\n",
    "\n",
    "languages_series = languages.set_index('Commune')['Language']\n",
    "\n",
    "#this function will be used inside the geojson method to color the part of the map as descibed by color_language\n",
    "def style_function_language(feature):\n",
    "    language = languages_series.get(feature['name'], None)\n",
    "    if(language == None):\n",
    "        print(feature['name'])\n",
    "    return {\n",
    "        'fillOpacity': 1,\n",
    "        'weight': 0,\n",
    "        'fillColor': color_language[language]\n",
    "    }\n",
    "\n",
    "m = folium.Map(\n",
    "    location=switzerland_coord,\n",
    "    tiles=None,\n",
    "    zoom_start=8\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    geo_json_data,\n",
    "    style_function=style_function_language\n",
    ").add_to(m)\n",
    "\n",
    "m.choropleth(geo_data=geo_json_data,\n",
    "             fill_opacity=0,\n",
    "             line_opacity=1)\n",
    "\n",
    "m.save('../docs/assets/data/map_language.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each language and intensity spoken we search for the 5 most voted and the 5 least voted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_by_l in data_lang.groupby('Language') :\n",
    "    current_language = data_by_l[0]\n",
    "    databl_mean = data_by_l[1].groupby('Votation' , as_index = False).mean()[['Votation','Oui en %']]\n",
    "    databl_votation = databl_mean.sort_values(by='Oui en %' , ascending = False)\n",
    "    print (current_language + ' max : '  )\n",
    "    print (databl_votation.head(5))\n",
    "    \n",
    "    print (current_language + ' min : '  )\n",
    "    print (databl_votation.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map by theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part creates maps for each that show the percentage of agreement for each thematique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = pd.read_csv(\"data/px-x-1703010000_103.csv\", sep=\";\", encoding=\"cp1254\", skiprows=2)[:-1]\n",
    "themes = themes[~themes['Période'].str.contains(\"bis\")]\n",
    "themes[\"Période\"] = themes[\"Période\"].apply(lambda x : x.split(\" \")[1])\n",
    "themes = themes.set_index(\"Période\")\n",
    "\n",
    "themes.columns = ['Political regime', 'Foreign policy' , 'security policy' , 'Economy' , 'public finances',\\\n",
    "                  'Infrastructure, planning, environment', 'social policies' , \\\n",
    "                 'Education,culture,media' ]\n",
    "\n",
    "themes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We extract the subjects of votations from the index of the data and make a dataframe we the subjects and the year\n",
    "df = pd.read_pickle(\"data/votations.pkl\")\n",
    "votations = pd.DataFrame(df.index.levels[1])\n",
    "votations[\"Année\"] = votations[\"Votation\"].apply(lambda s : s[6:10])\n",
    "votations = votations.sort_values(\"Année\")\n",
    "votations = votations.reset_index(drop=True)\n",
    "\n",
    "header = list(themes.columns)\n",
    "values = []\n",
    "\n",
    "#Now let's use our indexation of themes to append a theme to each votation. We use a handmade indexation of the\n",
    "#themes since no mapping between subjects and themes exist online. We used the available listing of voted themes\n",
    "#for each year and manually reattributed the themes to the subjects.\n",
    "with open(\"data/theme_indices.txt\", \"r\") as file:\n",
    "    for year in votations[\"Année\"].unique():\n",
    "        indices = file.readline().replace(\" \", \"\").split(\",\")\n",
    "        temp = []\n",
    "        \n",
    "        #We get the themes for the current year\n",
    "        for i in range(len(header)):\n",
    "            for j in range(int(themes.loc[year, header[i]])):\n",
    "                temp.append(header[i])\n",
    "\n",
    "        #We reorder them using our indexation and add them to the list\n",
    "        values += list(map(lambda x : temp[int(x) - 1], indices))\n",
    "        \n",
    "#We add the list to the dataframe as a column\n",
    "votations[\"Theme\"] = values\n",
    "votations = votations.drop(\"Année\", axis=1)\n",
    "votations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the 2 dataframes so that we have for each votation and each commune the theme and the percentage of yes. We only takes 'Theme','Commune' and 'Oui en %' because it will be the only usefull information for later (Votation is no longer usefull once we managed to merge)\n",
    "\n",
    "We also make sure that there is in the dataframes only commune that are in the json so that we do not make the folium functions crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thematique = votations.copy()\n",
    "data_theme = data.merge(thematique , on = 'Votation')\n",
    "\n",
    "data_t = data_theme[['Theme','Commune','Oui en %']]\n",
    "data_t = data_t[data_t['Commune'].isin(commune)]\n",
    "data_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the data by theme, and for each one we create a map showing how much people voted yes. We then save it into an html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for theme, curr_data_theme in data_t.groupby('Theme') :\n",
    "    curr_data_theme = curr_data_theme.groupby('Commune', as_index  = False).mean()\n",
    "    map1 = folium.Map(location=switzerland_coord, zoom_start=8)\n",
    "    map1.choropleth(geo_data = geo_json_data, \\\n",
    "                                    data = curr_data_theme, \\\n",
    "                                    columns = ['Commune', 'Oui en %'], \\\n",
    "                                    key_on = 'feature.name', \\\n",
    "                                    fill_color = 'YlGnBu', \\\n",
    "                                    fill_opacity = 0.7, \\\n",
    "                                    line_opacity = 0.2, \\\n",
    "                                    legend_name = 'yes in % given to the theme ' + theme)\n",
    "    \n",
    "    map1.save('../docs/assets/data/map_theme/map_'+theme+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map by recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for using the proposition of vote of each poilitical party to create a visual representation of how much each party is listened and try to see which region vote more for each party."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part load the dataframe with the % of yes and clean the dataframe so that we have only the date in one column and another to have only the subject of the votation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataframe with the votation data\n",
    "df = data.copy()\n",
    "df['Date'] = df['Votation'].map(lambda x : x.split(' ')[0])\n",
    "#df['Votation'] = df['Votation'].map(lambda x : ' '.join(x.split(' ')[1:]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download all the page \n",
    "xls = pd.ExcelFile('data/Recommandations des partis.xls')\n",
    "#the dataframe to fill with the info of the recommandation\n",
    "recommend = pd.DataFrame()\n",
    "\n",
    "#for every page with information from 2017 to 1981  :\n",
    "for i in range ((2017 - 1981 + 1)) :\n",
    "    #get the page i\n",
    "    x = xls.parse(i)\n",
    "    #change the name of the columns so that they will be easier to use \n",
    "    x.columns = range(len(x.columns))\n",
    "    \n",
    "    #We will need the parties and the numero of the votation, for that we use the fact that the line that have the date \n",
    "    #always begin by 'Parti 1)' and that the no of votation is 2 lines later\n",
    "    base_nb = x[x.iloc[:, 0] == 'Parti 1)'].index[0]\n",
    "    \n",
    "    #We drop all the columns with only Nan\n",
    "    x = x.dropna(axis=1, how='all')\n",
    "\n",
    "    #concat the date, the No of votation and the conseil of vote using the fact that they are the only lines with Nan\n",
    "    recommend_inter = pd.concat([x.iloc[base_nb:base_nb + 1], x.iloc[base_nb + 2:base_nb + 3], x.dropna()]).transpose()\n",
    "    \n",
    "    #this part change the name of the party so that even if there is little change in the name  (like had a 3) to the name)\n",
    "    #we still have consistent name\n",
    "    recommend_inter.iloc[0, 2:] = recommend_inter.iloc[0, 2:].map(lambda x : x.split(' ')[0])    \n",
    "    \n",
    "    #change the name of the columns so that they will be easier to use\n",
    "    recommend_inter.columns = range(len(recommend_inter.columns))\n",
    "    \n",
    "    recommend_inter.iloc[:, 0] = recommend_inter.iloc[:, 0] + str(2017 - i)\n",
    "    # make sure that 'no ###' are  made into 'No ###'\n",
    "    recommend_inter.iloc[:, 1] = recommend_inter.iloc[:, 1].map(lambda x : x[0].upper() + x[1:], na_action='ignore')\n",
    "    \n",
    "    #get the lines that give the name of a votation knowing its No\n",
    "    propositions = x[x.iloc[:, 0].str.contains('No ').fillna(False)]\n",
    "    #change the name of the columns so that they will be easier to use\n",
    "    propositions.columns = range(len(propositions.columns))\n",
    "    \n",
    "    #The next part is transforming the No of votation into name of votation\n",
    "    \n",
    "    #in some sheet this information is on only 1 cell so we split the information in 2 clls like the rest of the sheets \n",
    "    if (propositions.iloc[:, 1].isnull()).all():\n",
    "        if (2017 - i == 1997):\n",
    "            propositions.iloc[:, 1] = propositions[0].map(lambda x : x.split(' ')[1])\n",
    "            propositions.iloc[:, 0] = propositions[0].map(lambda x : x.split(' ')[0].rstrip())\n",
    "        else:\n",
    "            propositions.iloc[:, 1] = propositions[0].map(lambda x : x.split(':')[1][1:])\n",
    "            propositions.iloc[:, 0] = propositions[0].map(lambda x : x.split(':')[0].rstrip())\n",
    "    \n",
    "    \n",
    "    dico_no_propos = propositions.dropna(1).set_index(0).to_dict()\n",
    "    \n",
    "    #some dico are inside a dictionarry {1:{true_dictionarry}}\n",
    "    if (dico_no_propos.get(1) == None) :\n",
    "        recommend_inter.iloc[:, 1] = recommend_inter.iloc[:, 1].map(dico_no_propos)\n",
    "    else :\n",
    "        recommend_inter.iloc[:, 1] = recommend_inter.iloc[:, 1].map(dico_no_propos[1])\n",
    "        \n",
    "    #rename column meaningfully\n",
    "    recommend_columns = ['Date', 'Votation'] + list(recommend_inter.iloc[0][2:])\n",
    "    recommend_inter.columns = recommend_columns\n",
    "    \n",
    "    #fill the date for the line that does not have it\n",
    "    recommend_inter['Date'] = recommend_inter['Date'].fillna(method='ffill')\n",
    "    #transpose so that we can join\n",
    "    recommend_inter = recommend_inter.transpose()\n",
    "    \n",
    "    #delete duplicate (in some sheet there is twice PST )\n",
    "    recommend_inter = recommend_inter.groupby(recommend_inter.index).first()\n",
    "    #join the 2 dataframe, the name of the column are not important\n",
    "    recommend = recommend.join(recommend_inter, how='outer', lsuffix='l', rsuffix='r')\n",
    "\n",
    "recommend = recommend.transpose()  \n",
    "\n",
    "#clean the dataframe \n",
    "recommend = recommend[~recommend.loc[:, 'Date'].str.contains('Parti 1').fillna(False)]\n",
    "recommend = recommend[~recommend['Votation'].isnull()]\n",
    "#create meaningfull index\n",
    "recommend.index = range(len(recommend.index))\n",
    "\n",
    "#transform the date into the same format than the main dataframe\n",
    "month_to_int = { \\\n",
    "    'janvier' : '01', \\\n",
    "    'février' : '02', \\\n",
    "    'févirer' : '02', \\\n",
    "    'mars' : '03', \\\n",
    "    'avril' : '04', \\\n",
    "    'mai' : '05', \\\n",
    "    'juin' : '06', \\\n",
    "    'juillet' : '07', \\\n",
    "    'aout' : '08', \\\n",
    "    'septembre' : '09', \\\n",
    "    'octobre' : '10', \\\n",
    "    'novembre' : '11', \\\n",
    "    'décembre' : '12' \\\n",
    "}\n",
    "\n",
    "def good_format_month (x) :\n",
    "    if '.' in x :\n",
    "        split_x = x.split('.')\n",
    "        return split_x[0].zfill(2) + '.' + split_x[1].zfill(2) + '.' + split_x[2][:4]\n",
    "    else :\n",
    "        return str(x.split(' ')[0]).zfill(2) + '.' +  month_to_int[str(x.split(' ')[1]) ] + '.' + x[-4:] \n",
    "\n",
    "recommend.loc[:, 'Date'] = recommend.loc[:, 'Date'].map(lambda x : good_format_month(x))\n",
    "\n",
    "recommend = recommend.loc[:, recommend.columns.drop_duplicates() ]\n",
    "#parties is the list of all the parties\n",
    "parties = list(recommend.columns.drop_duplicates())\n",
    "parties.remove('Date')\n",
    "parties.remove('Votation')\n",
    "parties.remove('Parti')\n",
    "\n",
    "recommend = recommend.loc[:, ['Date', 'Votation'] + parties]\n",
    "recommend.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value so that they are all uniform       \n",
    "a yes recommandation becomes 1     \n",
    "a no recommandation becomes -1   \n",
    "No data or no recommendation is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_choice(x) :\n",
    "    if (x == 1 or str(x) == 'oui') :\n",
    "        return 1\n",
    "    elif (x == 2 or str(x) == 'non'):\n",
    "        return -1\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "good_recommend = recommend.copy() \n",
    "good_recommend.loc[:, parties] = good_recommend.loc[:, parties].applymap(lambda x : translate_choice(x))\n",
    "good_recommend.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a dictionarry to link the name of the votation in the recomendation dataframe and the vote dataframe.   \n",
    "The problem is that the name are very different, so to link one to another we need to group the votation of the 2 by date and for each date we compare all pair using SequenceMatch, we find the best match, we decide to link this pair together, we take out the 2 element from their respective list and we research a new maximum until there is no elemnt in one of the list.\n",
    "\n",
    "This method might create some imprecision but as a whole it is solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_recom_vote = {}\n",
    "\n",
    "for date1, group1 in df[df['Commune'] == 'Aeugst am Albis'].groupby('Date'):\n",
    "    \n",
    "    for date2, group2 in good_recommend.groupby('Date'):\n",
    "        \n",
    "        if (date1 == date2) :\n",
    "            vot1 = list(group1['Votation']) \n",
    "            vot2 = list(group2['Votation'])\n",
    "            \n",
    "            while (len(vot1) > 0 and len(vot2) > 0):\n",
    "                max_v = -1\n",
    "                max_match = [0, 0]\n",
    "                \n",
    "                for elem1 in vot1:\n",
    "                    for elem2 in vot2:\n",
    "                        current_v = SequenceMatcher(None, elem1, elem2).ratio()\n",
    "                        \n",
    "                        if (current_v > max_v) :\n",
    "                            max_v = current_v\n",
    "                            max_match = [elem1, elem2]\n",
    "                            \n",
    "                dico_recom_vote[max_match[0]] = max_match[1]\n",
    "                dico_recom_vote[max_match[1]] = max_match[0]\n",
    "                vot1.remove(max_match[0])\n",
    "                vot2.remove(max_match[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a recommandation dataframe with names corresponding to the ones in the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend = good_recommend.copy()\n",
    "recommend['Votation'] = recommend['Votation'].map(dico_recom_vote)\n",
    "recommend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parties = list(recommend.columns.drop_duplicates())\n",
    "parties.remove('Date')\n",
    "parties.remove('Votation')\n",
    "\n",
    "\n",
    "\n",
    "data_recommend = data.merge(recommend.loc[:, ['Votation'] + parties], on='Votation')\n",
    "data_recommend.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a map of % of people agreeing in a party for each party.\n",
    "\n",
    "People agreeing are considered to people that vote the same as the party if it votes yes or no. We do not take into consideration other proposition of the party (like abstentation) or when we do not have information about the recommandation of a party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for parti in parties :\n",
    "    current_to_map = data_recommend.loc[:, ['Commune', 'Oui en %', parti]]\n",
    "    current_to_map = current_to_map[current_to_map[parti] != 0]\n",
    "    current_to_map['Agreement'] = current_to_map[['Oui en %', parti]] \\\n",
    "        .apply(lambda x : x['Oui en %'] if x[parti] == 1 else 100 - x['Oui en %'], axis=1)\n",
    "    \n",
    "    current_to_map = current_to_map.groupby('Commune', as_index=False).mean()\n",
    "    \n",
    "    map1 = folium.Map(location=switzerland_coord, zoom_start=8)\n",
    "    map1.choropleth(geo_data = geo_json_data, \\\n",
    "                    data = current_to_map, \\\n",
    "                    columns = ['Commune', 'Agreement'], \\\n",
    "                    key_on = 'feature.name', \\\n",
    "                    fill_color = 'YlGnBu', \\\n",
    "                    fill_opacity = 0.7, \\\n",
    "                    line_opacity = 0.2, \\\n",
    "                    legend_name = 'Agreement in % with ' + parti)\n",
    "    \n",
    "    map1.save('../docs/assets/data/maps_partis/map_' + parti + '.html')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "draw_map_kmeans is a function that will cluster all the commune of Switzerland using a kmeans method with n_clusters group and the matrix X given, return a map that show all the commune in different colors depending their groups, and reduce the matrix X to 2 parameters using PCA to create a scatter plot graph that will be a graphical representation of this where one point will be one commune and if it has a value for file_PCA, it will create a png there otherwise it will just print this graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_map_kmeans (n_clusters , X , file_PCA = None ) :\n",
    "    #cluster using kmeans\n",
    "    kmeans_res = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    groups = kmeans_res.labels_\n",
    "    commune_to_group = pd.DataFrame({'Commune' : commune_list , 'Group' : groups})\n",
    "    commune_to_group = commune_to_group.set_index('Commune')['Group']\n",
    "    \n",
    "    \n",
    "    ##reduce matrix using PCA\n",
    "    plt.figure(100+n_clusters)\n",
    "    \n",
    "    \n",
    "    model_PCA = PCA ( n_components=2)\n",
    "    X_PCA = model_PCA.fit_transform(X)\n",
    "    \n",
    "    for current_group in range (n_clusters) :\n",
    "        group_y = [X_PCA[i] for i in range(len(X_PCA)) if groups[i] == current_group]\n",
    "        plt.scatter( [x[0] for x in group_y], [x[1] for x in group_y], c= color_list[current_group])\n",
    "    \n",
    "    if (file_PCA != None) :\n",
    "        plt.savefig(file_PCA+'PCAA_kmeans'+str(n_clusters)+'.png')\n",
    "    else :\n",
    "        print(plt.show())\n",
    "    plt.gcf().clear()\n",
    "    \n",
    "    \n",
    "    #create a function for how to color the map\n",
    "    def style_function_kmeans(feature):\n",
    "        group = commune_to_group.get(feature['name'], None)\n",
    "        if(group == None):\n",
    "            print(feature['name'])\n",
    "        return {\n",
    "            'fillOpacity': 1,\n",
    "            'weight': 0,\n",
    "            'fillColor': color_list[group]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    #create the map\n",
    "    m = folium.Map(location=switzerland_coord, zoom_start=8)\n",
    "    \n",
    "    folium.GeoJson(\n",
    "        geo_json_data,\n",
    "        style_function=style_function_kmeans\n",
    "    ).add_to(m)\n",
    "\n",
    "    m.choropleth(geo_data=geo_json_data,\n",
    "             fill_opacity=0,\n",
    "             line_opacity=1)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the map and PCA graph as describe above using draw_map_kmeans for kmeans with 2 to 5 groups and put them in data/map_ml\n",
    "for i in range (2,6) :  \n",
    "    draw_map_kmeans(i,X, file_PCA='../docs/assets/data/map_ml/').save('../docs/assets/data/map_ml/kmeans'+str(i)+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "draw_map_DBSCAN is a function that will cluster all the commune of Switzerland using a DBSCAN method and the matrix X given, return a map that show all the commune in different colors depending their groups, and reduce the matrix X to 2 parameters using PCA to create a scatter plot graph that will be a graphical representation of this where one point will be one commune and if it has a value for file_PCA, it will create a png there otherwise it will just print this graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_epsilon(X) :\n",
    "    min_samples = 2*len(X[0])\n",
    "    X_array = [ np.array(x_) for x_ in X]\n",
    "    range_X = range(len(X))\n",
    "    calculate = np.sort([ \\\n",
    "                            np.sort([np.linalg.norm(X_array[x]-X_array[y]) \\\n",
    "                             for x in range_X  if x!=y])[int(min_samples/2)] \\\n",
    "                           \\\n",
    "                   for y in range_X ] )\n",
    "    \n",
    "    \n",
    "    for i in range (len(calculate)) :\n",
    "        plt.scatter( i,calculate[i])\n",
    "    \n",
    "    print(plt.show())\n",
    "        \n",
    "    plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_map_DBSCAN (X , file_PCA = None) : \n",
    "    #cluster using DBSCAN\n",
    "    min_samples =  20\n",
    "    \n",
    "    \n",
    "\n",
    "    X_array = [ np.array(x_) for x_ in X]\n",
    "    range_X = range(len(X))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Xmeans = np.mean([ np.mean(\\\n",
    "                            np.sort([np.linalg.norm(X_array[x]-X_array[y]) \\\n",
    "                             for x in range_X  if x!=y])[:(min_samples*2-1)] \\\n",
    "                           )\\\n",
    "                   for y in range_X ] )\n",
    "    groups =  DBSCAN(eps=Xmeans, min_samples=min_samples).fit(X).labels_\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##reduce matrix using PCA\n",
    "    model_PCA = PCA ( n_components=2)\n",
    "    X_PCA = model_PCA.fit_transform(X)\n",
    "    \n",
    "    n_clusters = max(groups)+1\n",
    "    for current_group in range (-1,n_clusters) :\n",
    "        group_y = [X_PCA[i] for i in range(len(X_PCA)) if groups[i] == current_group]\n",
    "        plt.scatter( [x[0] for x in group_y], [x[1] for x in group_y], c= color_list[current_group])\n",
    "    \n",
    "    \n",
    "    if (file_PCA != None) :\n",
    "        \n",
    "        plt.savefig(file_PCA+'PCAA_DBSCAN.png')\n",
    "    else :\n",
    "        print(plt.show())\n",
    "        \n",
    "    plt.gcf().clear()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #create a function for how to color the map\n",
    "    commune_to_group = pd.DataFrame({'Commune' : commune_list , 'Group' : groups}).set_index('Commune')['Group']\n",
    "    \n",
    "    def style_function_DBSCAN(feature):\n",
    "        group = commune_to_group.get(feature['name'], None)\n",
    "        if(group == None):\n",
    "            print(feature['name'])\n",
    "        return {\n",
    "            'fillOpacity': 1,\n",
    "            'weight': 0,\n",
    "            'fillColor': color_list[group]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    #create the map\n",
    "    m = folium.Map(location=switzerland_coord, zoom_start=8)\n",
    "    \n",
    "    folium.GeoJson(\n",
    "        geo_json_data,\n",
    "        style_function=style_function_DBSCAN\n",
    "    ).add_to(m)\n",
    "\n",
    "    m.choropleth(geo_data=geo_json_data,\n",
    "             fill_opacity=0,\n",
    "             line_opacity=1)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draw_map_DBSCAN (X,'../docs/assets/data/map_ml/').save('../docs/assets/data/map_ml/DBSCAN.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering for theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell I will cluster again the data but using only votes from one theme at a time, and this for all theme.\n",
    "The method used to cluster will be DBSCAN and kmeans with 2 cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_bg = data by group\n",
    "for data_bg in data_theme[['Theme','Votation','Commune','Oui en %']].groupby('Theme') :\n",
    "    theme = data_bg[0]\n",
    "    data_bg = pd.DataFrame(data = data_bg[1])\n",
    "\n",
    "    votation_list_t = list(set(data_bg['Votation'].values))\n",
    "    votation_dict_t = { val : idx for idx , val in enumerate(votation_list_t)   }\n",
    "\n",
    "    Xt = np.ones((len(commune_list) , len(votation_list_t) ) , dtype=float)\n",
    "    \n",
    "    for x in data_bg [['Commune','Votation','Oui en %']].fillna(50).values :\n",
    "        Xt [commune_dict[x[0]]][votation_dict_t[x[1]]] = x[2]\n",
    "    draw_map_kmeans(2,Xt,'../docs/assets/data/maps_theme_ml/'+theme).save('../docs/assets/data/maps_theme_ml/kmeans_'+theme+'.html') \n",
    "    draw_map_DBSCAN (Xt,'../docs/assets/data/maps_theme_ml/'+theme).save('../docs/assets/data/maps_theme_ml/DBSCAN_'+theme+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster by recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell I will cluster again the data but this time instead of using the percentage of yes vote I will use the percentage of agreement with each parties.\n",
    "The method used to cluster will be DBSCAN and kmeans with 2 cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parti in parties :\n",
    "    curr_recommend = data_recommend.loc[:, ['Commune', 'Votation' , 'Oui en %', parti]]\n",
    "    curr_recommend = curr_recommend[curr_recommend[parti] != 0]\n",
    "    curr_recommend['Agreement'] = curr_recommend[['Oui en %', parti]] \\\n",
    "        .apply(lambda x : x['Oui en %'] if x[parti] == 1 else 100 - x['Oui en %'], axis=1)\n",
    "    \n",
    "    \n",
    "    votation_list_t = list(set(curr_recommend['Votation'].values))\n",
    "    votation_dict_t = { val : idx for idx , val in enumerate(votation_list_t)   }\n",
    "\n",
    "    Xt = np.ones((len(commune_list) , len(votation_list_t) ) , dtype=float)\n",
    "    \n",
    "    for x in curr_recommend [['Commune','Votation','Oui en %']].fillna(50).values :\n",
    "        Xt [commune_dict[x[0]]][votation_dict_t[x[1]]] = x[2]\n",
    "    draw_map_kmeans(2,Xt,'../docs/assets/data/map_recommendation_cluster/'+parti).save('../docs/assets/data/map_recommendation_cluster/kmeans_'+parti+'.html') \n",
    "    draw_map_DBSCAN (Xt,'../docs/assets/data/map_recommendation_cluster/'+parti).save('../docs/assets/data/map_recommendation_cluster/DBSCAN_'+parti+'.html')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current day analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do the analysis of the clustering since 2005 (which is a good compromise between being enough in the present and having enough data to create meaningfull clusters). \n",
    "We will use only clustering because they are the only method which made meaningfull results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_date (x) :\n",
    "    date = x.split(' ') [0]\n",
    "    return '.'.join(date.split('.')[::-1])\n",
    "\n",
    "\n",
    "#return -1 if date1 < date2 return 1 if date1 > date2 return 0 if if date1 == date2\n",
    "def compare_date (date1 , date2) :\n",
    "    date1_s = date1.split('.')\n",
    "    date2_s = date2.split('.')\n",
    "    for i in range ( len(date1_s)) :\n",
    "        if ( int(date1_s[i]) < int(date2_s[i])) :\n",
    "            return -1\n",
    "        elif ( int(date1_s[i]) > int(date2_s[i])) :\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "data['Date'] = data['Votation'].map( get_good_date)\n",
    "\n",
    "\n",
    "data = data [ data ['Date'].map (lambda x : compare_date ( '2005.00.00' , x ) == -1 )]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_recommend = data.merge(recommend.loc[:, ['Votation'] + parties], on='Votation')\n",
    "data_theme = data.merge(thematique , on = 'Votation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votation_list = list(set(data['Votation'].values))\n",
    "votation_dict = { val : idx for idx , val in enumerate(votation_list)   }\n",
    "\n",
    "X = np.ones((len(commune_list) , len(votation_list) ) , dtype=float)\n",
    "\n",
    "\n",
    "for x in data [['Commune','Votation','Oui en %']].fillna(50).values :\n",
    "    X [commune_dict[x[0]]][votation_dict[x[1]]] = x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (2,6) :  \n",
    "    draw_map_kmeans(i,X, file_PCA='../docs/assets/data/young/map_ml/').save('../docs/assets/data/young/map_ml/kmeans'+str(i)+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_map_DBSCAN (X,'../docs/assets/data/young/map_ml/').save('../docs/assets/data/young/map_ml/DBSCAN.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_bg = data by group\n",
    "for data_bg in data_theme[['Theme','Votation','Commune','Oui en %']].groupby('Theme') :\n",
    "    theme = data_bg[0]\n",
    "    data_bg = pd.DataFrame(data = data_bg[1])\n",
    "\n",
    "    votation_list_t = list(set(data_bg['Votation'].values))\n",
    "    votation_dict_t = { val : idx for idx , val in enumerate(votation_list_t)   }\n",
    "\n",
    "    Xt = np.ones((len(commune_list) , len(votation_list_t) ) , dtype=float)\n",
    "    \n",
    "    for x in data_bg [['Commune','Votation','Oui en %']].fillna(50).values :\n",
    "        Xt [commune_dict[x[0]]][votation_dict_t[x[1]]] = x[2]\n",
    "    draw_map_kmeans(2,Xt,'../docs/assets/data/young/maps_theme_ml/'+theme).save('../docs/assets/data/young/maps_theme_ml/kmeans_'+theme+'.html') \n",
    "    draw_map_DBSCAN (Xt,'../docs/assets/data/young/maps_theme_ml/'+theme).save('../docs/assets/data/young/maps_theme_ml/DBSCAN_'+theme+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parti in parties :\n",
    "    curr_recommend = data_recommend.loc[:, ['Commune', 'Votation' , 'Oui en %', parti]]\n",
    "    curr_recommend = curr_recommend[curr_recommend[parti] != 0]\n",
    "    if (len(curr_recommend) > 0):\n",
    "        curr_recommend['Agreement'] = curr_recommend[['Oui en %', parti]] \\\n",
    "            .apply(lambda x : x['Oui en %'] if x[parti] == 1 else 100 - x['Oui en %'], axis=1)\n",
    "    \n",
    "    \n",
    "        votation_list_t = list(set(curr_recommend['Votation'].values))\n",
    "        votation_dict_t = { val : idx for idx , val in enumerate(votation_list_t)   }\n",
    "\n",
    "        Xt = np.ones((len(commune_list) , len(votation_list_t) ) , dtype=float)\n",
    "    \n",
    "        for x in curr_recommend [['Commune','Votation','Oui en %']].fillna(50).values :\n",
    "            Xt [commune_dict[x[0]]][votation_dict_t[x[1]]] = x[2]\n",
    "        \n",
    "        if ( len(Xt[0]) > 1) :\n",
    "            draw_map_kmeans(2,Xt,'../docs/assets/data/young/map_recommendation_cluster/'+parti).save('../docs/assets/data/young/map_recommendation_cluster/kmeans_'+parti+'.html') \n",
    "            draw_map_DBSCAN (Xt,'../docs/assets/data/young/map_recommendation_cluster/'+parti).save('../docs/assets/data/young/map_recommendation_cluster/DBSCAN_'+parti+'.html')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "path=\"C:\\\\Users\\\\Devaux Matthieu\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe\"\n",
    "browser=webdriver.Chrome(path)\n",
    "delay=5\n",
    "fn='testmap.html'\n",
    "\n",
    "def to_png(html) :\n",
    "    tmpurl=('file:///D:/epfl/ADA2017/docs/'+html).format(path=os.getcwd(),mapfile=fn)\n",
    "\n",
    "\n",
    "\n",
    "    browser.get(tmpurl)\n",
    "    #Give the map tiles some time to load\n",
    "    time.sleep(delay)\n",
    "    browser.save_screenshot('.'.join(html.split('.')[0:-1])+'.png')\n",
    "    \n",
    "\n",
    "\n",
    "def iter_png (path) :\n",
    "    for fn in os.listdir(path):\n",
    "         if  os.path.isfile(path+'/'+fn)  :\n",
    "            if fn.split('.')[-1]=='html' :\n",
    "                to_png(path+'/'+fn)\n",
    "         else :\n",
    "            if path =='.' :\n",
    "                iter_png (fn)\n",
    "            else :\n",
    "                iter_png (path+'/'+fn)\n",
    "         \n",
    "iter_png('../docs/assets/data')\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
